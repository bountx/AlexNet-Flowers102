{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ulhVYI86xhZ"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "**Submission deadlines:**\n",
        "\n",
        "  - Tuesday groups: 22.04.2025\n",
        "  - Friday groups: 18.04.2025\n",
        "\n",
        "**Points:** Aim to get 10 points + 4 extra\n",
        "\n",
        "## Submission instructions\n",
        "The class is held on-site in lab rooms. Please prepare you notebook on your computer or anywhere in the cloud (try using DeepNote or Google Colab).\n",
        "\n",
        "Make sure you know all the questions and answers, and that the notebook contains results; before presentation do `Runtime -> Restart and run all`\n",
        "\n",
        "![Alt text](data:image/png;base64,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)\n",
        "\n",
        "We provide starter code, however you are not required to use it as long as you properly solve the tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pq455pMGbe2"
      },
      "source": [
        "# Classify the Oxford Flowers dataset (Weight & Biases) [6p]\n",
        "\n",
        "In this task, you will train a convolutional neural network to classify images of flowers from the [Oxford Flowers 102 dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). The dataset consists of 102 flower categories, and each class has between 40 and 258 images. The images have large scale, pose, and light variations. In addition, there are categories that have large variations within the category and several very similar categories.\n",
        "\n",
        "    \n",
        "The dataset is available in `torchvision.datasets.Flowers102` class; see [Flowers102.html](https://pytorch.org/vision/main/generated/torchvision.datasets.Flowers102.html). You can use the following code to load the dataset:\n",
        "\n",
        "```python\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "dataset = torchvision.datasets.Flowers102(root='./data', download=True, transform=transforms.ToTensor())\n",
        "```\n",
        "\n",
        "**Hint**: The default split of the dataset is 1020, 1020 and 6149 images for training, validation and test sets respectively.\n",
        "If you can handle the bigger training dataset, you can experiment by taking up to 80% of the test set for training.\n",
        "\n",
        "\n",
        "In this task you should run several experiments to classify the images.\n",
        "In order to track the experiments, you can use the `Weight & Biases` library; see the [documentation](https://docs.wandb.ai/quickstart) for more details.\n",
        "\n",
        "Implement your code as a single Python script or a Jupyter notebook. Remember to log the experiment configuration, hyperparameters, and results (e.g., training loss, validation loss, accuracy and test loss, accuracy).\n",
        "For logging, you can use the `wandb.log` function to log the metrics and hyperparameters. You can also log the model architecture, training curves, and other relevant information.\n",
        "\n",
        "* 1. **[1p]**:\n",
        "    * Your task is to implement a convolutional neural network from scratch using PyTorch.\n",
        "    * Your CNN should consist of convolutional layers (Conv2D), pooling layers (MaxPooling2D), activation layers (e.g., ReLU), and fully connected layers (if needed).\n",
        "    \n",
        "* 2. **[2p]**:\n",
        "    * Train your CNN on different training set sized (10%, 20%, 50%, 80%, 100%) and evaluate the performance on the validation set and test set.\n",
        "        * Report the accuracy and loss on the validation set and test set for each training set size.\n",
        "    * Train your CNN on the full training set plus 20%, 50% and 80% of the test set and evaluate the performance on the validation set and the remaining test set.\n",
        "        * Report the accuracy and loss on the validation set and remaining test set for each training set size.\n",
        "    * Compare the performance of your CNN on the different training set sizes and analyze the results.\n",
        "\n",
        "* 3. **[1p]**:\n",
        "    * Implement a baseline AlexNet model using PyTorch.\n",
        "    * Training AlexNet may take a long time, so try to use GPU acceleration if available.\n",
        "\n",
        "* 4. **[1p]**:\n",
        "    * Input normalization: experiment with different input normalization techniques (e.g., mean subtraction, standardization) and analyze their impact on the model's performance.\n",
        "\n",
        "* 5. **[2p]**:\n",
        "    * Experiment with different hyperparameters such as learning rate, batch size, number of epochs, and optimizer choice (e.g., SGD, Adam).\n",
        "\n",
        "* 6. **[2p]**:\n",
        "    * Modify your CNN architecture to include batch normalization and dropout layers.\n",
        "    * Experiment with different dropout rates and analyze their impact on the model's performance.\n",
        "\n",
        "* 7. **[1p]**:\n",
        "    * Implement data augmentation techniques such as random rotations, shifts, flips, and zooms on the training dataset.\n",
        "    * Train your CNN with augmented data and compare the performance with the baseline model trained on the original data.\n",
        "\n",
        "* 8. ***[2p extra points]***:\n",
        "    * Implement residual connections in your CNN architecture; see the [ResNet paper](https://arxiv.org/abs/1512.03385) for more details.\n",
        "    * Implement inception modules in your CNN architecture; see the [GoogLeNet paper](https://arxiv.org/abs/1409.4842) for more details.\n",
        "                \n",
        "\n",
        "Analyze the results obtained from different experiments.\n",
        "Discuss the effects of varying training set size, hyperparameters, batch normalization, dropout, and data augmentation on the CNN's performance.\n",
        "Provide insights into how these factors influence model training, convergence, and generalization.\n",
        "\n",
        "Use the `Weight & Biases` reports to present your findings in a comprehensive report or presentation; see the [documentation](https://docs.wandb.ai/quickstart) for more details.\n",
        "\n",
        "**[2p extra]**: present your findings (for each task) in a report format in Weight & Biases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWysa90FtFwQ"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWjVEdM3OmXk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUXHbRSUwn0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTGGPE56Ja9A"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cNAKDxm1lCb"
      },
      "outputs": [],
      "source": [
        "def create_subset(dataset, percentage, seed=42):\n",
        "    \"\"\"Create reproducible subset\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    size = int(len(dataset) * percentage)\n",
        "    indices = np.random.choice(len(dataset), size, replace=False)\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def print_dataset_sizes(train, val, test):\n",
        "    print(f\"Training samples: {len(train):,}\")\n",
        "    print(f\"Validation samples: {len(val):,}\")\n",
        "    print(f\"Test samples: {len(test):,}\\n\")\n",
        "\n",
        "def split_dataset(dataset, train_ratio=0.8, seed=42):\n",
        "    \"\"\"Guarantees non-overlapping splits\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "    split_idx = int(len(dataset) * train_ratio)\n",
        "    return (\n",
        "        Subset(dataset, indices[:split_idx]),  # Train portion\n",
        "        Subset(dataset, indices[split_idx:])   # Test portion\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuXUXl0JJL4T"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOvVxS3cxLsT"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "def train_model(model, optimizer, train_loader, val_loader, test_loader, experiment_name, group, config, criterion=nn.CrossEntropyLoss(), additional_tags=[]):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_loader (DataLoader): Training data loader\n",
        "        val_loader (DataLoader): Validation data loader\n",
        "        test_loader (DataLoader): Test data loader\n",
        "        experiment_name (str): Identifier for weights and biases\n",
        "        group (str): Experiment group name\n",
        "        config (dict): Experiment configuration - hyperparameters\n",
        "        criterion (nn.Module): Loss function\n",
        "        additional_tags (list): Additional tags for weights and biases\n",
        "    \"\"\"\n",
        "\n",
        "    print(config)\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"flowers102-classification\",\n",
        "        config=config,\n",
        "        name=experiment_name,\n",
        "        group=group,\n",
        "        tags=[\"manual_run\", experiment_name] +additional_tags\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stop_counter = 0\n",
        "    early_stop_patience = config.get('early_stop_patience', 10)\n",
        "\n",
        "    for epoch in range(wandb.config.epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stop_counter = 0\n",
        "            torch.save(model.state_dict(), f\"best_model_{experiment_name}.pth\")\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"best_val_acc\": best_val_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{wandb.config.epochs} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if early_stop_counter >= early_stop_patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
        "            break\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "    wandb.log({\n",
        "        \"final_test_loss\": test_loss,\n",
        "        \"final_test_acc\": test_acc\n",
        "    })\n",
        "\n",
        "    print(f\"\\nExperiment '{experiment_name}' completed:\")\n",
        "    print(f\"Best Val Acc: {best_val_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    wandb.finish()\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaPzFBOfJtpY"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K7QsAn-Jwaa"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model (nn.Module): Trained model\n",
        "        loader (DataLoader): Data loader for evaluation\n",
        "        criterion: Loss function\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = loss / len(loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx3chYYYOrLv"
      },
      "source": [
        "## AlexNet baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNsT6gCOz7qy"
      },
      "outputs": [],
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self,num_classes = 102, dropout=0.5):\n",
        "        super(AlexNet,self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3,96, kernel_size=11,stride=4,padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2))\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "                nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(6400, 4096),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv-IGEcDX0r4"
      },
      "source": [
        "## Base configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_6Lj5yZYQIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4661fab-418a-4679-b299-aa69e835c790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:15<00:00, 22.1MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 1.06MB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 23.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "base_config_sgd = {\n",
        "    'epochs': 25,\n",
        "    'early_stop_patience': 5,\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 1e-3,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 0,\n",
        "    'dropout_p': 0.5,\n",
        "}\n",
        "\n",
        "base_config_adam = {\n",
        "    'epochs': 25,\n",
        "    'early_stop_patience': 5,\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 0,\n",
        "    'dropout_p': 0.5,\n",
        "}\n",
        "\n",
        "base_transform_alex = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "base_train_alex = datasets.Flowers102(root='./data', split='train', transform=base_transform_alex, download=True)\n",
        "base_val_alex = datasets.Flowers102(root='./data', split='val', transform=base_transform_alex)\n",
        "base_test_alex = datasets.Flowers102(root='./data', split='test', transform=base_transform_alex)\n",
        "\n",
        "base_train_loader_alex = DataLoader(base_train_alex, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "base_val_loader_alex = DataLoader(base_val_alex, batch_size=128, num_workers=2, pin_memory=True)\n",
        "base_test_loader_alex = DataLoader(base_test_alex, batch_size=128, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFrt8eD3OhqP"
      },
      "source": [
        "# Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaaY23Smz1nV"
      },
      "source": [
        "## Problem 1,2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFb5bFiVwrNe"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_train = datasets.Flowers102(root='./data', split='train',\n",
        "                               transform=train_transform, download=True)\n",
        "val_set = datasets.Flowers102(root='./data', split='val',\n",
        "                            transform=train_transform)\n",
        "full_test = datasets.Flowers102(root='./data', split='test',\n",
        "                              transform=train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xve9j_6zw7ro"
      },
      "outputs": [],
      "source": [
        "class FlowerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.AdaptiveAvgPool2d((7, 7))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnwF_lfyxmm7",
        "outputId": "7b7f7873-a1fc-425e-ab8f-991770dbe077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1,020\n",
            "Validation samples: 1,020\n",
            "Test samples: 6,149\n",
            "\n",
            "Training samples: 5,939\n",
            "Validation samples: 1,020\n",
            "Test samples: 1,230\n",
            "\n"
          ]
        }
      ],
      "source": [
        "base_config = {\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 20,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "exp1_train = create_subset(full_train, 1)\n",
        "exp1_test = full_test\n",
        "print_dataset_sizes(exp1_train, val_set, exp1_test)\n",
        "model = FlowerCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=base_config['learning_rate'])\n",
        "\n",
        "# ! Execute with:\n",
        "# train_model(model,\n",
        "#            DataLoader(exp1_train, batch_size=32, shuffle=True),\n",
        "#            optimizer,\n",
        "#            DataLoader(val_set, batch_size=32),\n",
        "#            DataLoader(exp1_test, batch_size=32),\n",
        "#            \"100%\",\n",
        "#            base_config)\n",
        "\n",
        "test_train, test_test = split_dataset(full_test, train_ratio=0.8)\n",
        "exp2_train = ConcatDataset([full_train, test_train])\n",
        "exp2_test = test_test\n",
        "print_dataset_sizes(exp2_train, val_set, exp2_test)\n",
        "\n",
        "# ! Execute with:\n",
        "# train_model(model,\n",
        "#            optimizer,\n",
        "#            DataLoader(exp2_train, batch_size=32, shuffle=True),\n",
        "#            DataLoader(val_set, batch_size=32),\n",
        "#            DataLoader(exp2_test, batch_size=32),\n",
        "#            '100%+50%test',\n",
        "#             base_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4cLr9_TTJqm"
      },
      "source": [
        "# Testing Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZNe8ypSVsuo"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOUFv1G9VpM2"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-1, 1e-2, 1e-3]\n",
        "learning_rates_adam = [1e-3, 1e-4, 1e-5]\n",
        "learning_rates_longer = [1e-2, 1e-3]\n",
        "learning_rates_adam_longer = [1e-3, 1e-4, 1e-5]\n",
        "\n",
        "lr_experiments = []\n",
        "for lr in learning_rates:\n",
        "    lr_config = base_config_sgd.copy()\n",
        "    lr_config['learning_rate'] = lr\n",
        "    lr_experiments.append({\n",
        "        'experiment_name': f'Alex_lr_{lr}',\n",
        "        'group': 'learning_rate',\n",
        "        'additional_tags': ['lr_test'],\n",
        "        'lr_config': lr_config,\n",
        "        })\n",
        "\n",
        "\n",
        "# Adam lr_experiments\n",
        "lr_adam_experiments = []\n",
        "for lr in learning_rates_adam:\n",
        "    lr_config = base_config_adam.copy()\n",
        "    lr_config['learning_rate'] = lr\n",
        "    lr_adam_experiments.append({\n",
        "        'experiment_name': f'Alex_lr_adam_{lr}',\n",
        "        'group': 'learning_rate',\n",
        "        'additional_tags': ['lr_test', 'adam'],\n",
        "        'lr_config': lr_config,\n",
        "        })\n",
        "\n",
        "\n",
        "# lr_experiments for 50 max epochs\n",
        "lr_more_epochs_experiments = []\n",
        "for lr in learning_rates_longer:\n",
        "    lr_config = base_config_sgd.copy()\n",
        "    lr_config['learning_rate'] = lr\n",
        "    lr_config['epochs'] = 50\n",
        "    lr_more_epochs_experiments.append({\n",
        "        'experiment_name': f'Alex_lr_longer_{lr}',\n",
        "        'group': 'learning_rate',\n",
        "        'additional_tags': ['lr_test', 'longer'],\n",
        "        'lr_config': lr_config,\n",
        "        })\n",
        "\n",
        "# lr_adam_experiments for 50 max epochs\n",
        "lr_adam_more_epochs_experiments = []\n",
        "for lr in learning_rates_adam_longer:\n",
        "    lr_config = base_config_adam.copy()\n",
        "    lr_config['learning_rate'] = lr\n",
        "    lr_config['epochs'] = 50\n",
        "    lr_adam_more_epochs_experiments.append({\n",
        "        'experiment_name': f'Alex_lr_adam_longer_{lr}',\n",
        "        'group': 'learning_rate',\n",
        "        'additional_tags': ['lr_test', 'adam', 'longer'],\n",
        "        'lr_config': lr_config,\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_g1k_qJBji_"
      },
      "source": [
        "## Dropout rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paEG2FuCBzaW"
      },
      "outputs": [],
      "source": [
        "# As of findings from Learning rate\n",
        "base_config_adam['learning_rate']=1e-4\n",
        "base_config_adam['epochs']=50\n",
        "\n",
        "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5, 0.6, 0.75]\n",
        "\n",
        "dr_experiments = []\n",
        "for dr in dropout_rates:\n",
        "    dr_config = base_config_adam.copy()\n",
        "    dr_config['dropout_p'] = dr\n",
        "    dr_experiments.append({\n",
        "        'experiment_name': f'Alex_dropout_{dr}',\n",
        "        'group': 'dropout',\n",
        "        'additional_tags': ['dropout_test'],\n",
        "        'model': model,\n",
        "        'optimizer': optimizer,\n",
        "        'dr_config': dr_config,\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight decay"
      ],
      "metadata": {
        "id": "VB0ZJsGkmNJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As of findings from Dropout rate\n",
        "base_config_adam['weight_decay'] = 1e-3\n",
        "base_config_adam['epochs'] = 100\n",
        "base_config_adam['early_stop_patience']= 10\n",
        "\n",
        "weight_decays = [0.1, 0, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "wd_experiments = []\n",
        "for wd in weight_decays:\n",
        "    wd_config = base_config_adam.copy()\n",
        "    wd_config['weight_decay'] = wd\n",
        "    wd_experiments.append({\n",
        "        'experiment_name': f'Alex_wd_{wd}',\n",
        "        'group': 'weight_decay',\n",
        "        'additional_tags': ['wd_test'],\n",
        "        'model': model,\n",
        "        'optimizer': optimizer,\n",
        "        'wd_config': wd_config,\n",
        "        })"
      ],
      "metadata": {
        "id": "IVzaCpCymMpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation"
      ],
      "metadata": {
        "id": "ixOshKnN1Mka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As of weight decay findings\n",
        "base_config_adam['weight_decay'] = 1e-2\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmented_train_alex = datasets.Flowers102(root='./data', split='train', transform=train_transform, download=True)\n",
        "augmented_train_loader_alex = DataLoader(augmented_train_alex, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "u52rhQdX1RAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hktqbNjudIXZ"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8kQ3-7ELdTez",
        "outputId": "dceb6383-659c-49c8-f37d-f658a85e55ea",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 50, 'early_stop_patience': 5, 'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0, 'dropout_p': 0.3}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrzej-pijanowski\u001b[0m (\u001b[33mandrzej-pijanowski-uniwroc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_151335-kdr8jvtm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/kdr8jvtm' target=\"_blank\">Alex_wd_0</a></strong> to <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/kdr8jvtm' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/kdr8jvtm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Train Loss: 4.6527 | Train Acc: 1.18% | Val Loss: 4.5909 | Val Acc: 4.71%\n",
            "Epoch 2/50 | Train Loss: 4.2130 | Train Acc: 6.27% | Val Loss: 4.2529 | Val Acc: 8.14%\n",
            "Epoch 3/50 | Train Loss: 3.7155 | Train Acc: 13.33% | Val Loss: 3.8558 | Val Acc: 12.25%\n",
            "Epoch 4/50 | Train Loss: 3.2663 | Train Acc: 20.00% | Val Loss: 3.6112 | Val Acc: 12.06%\n",
            "Epoch 5/50 | Train Loss: 2.9128 | Train Acc: 27.35% | Val Loss: 3.5318 | Val Acc: 16.18%\n",
            "Epoch 6/50 | Train Loss: 2.5877 | Train Acc: 33.04% | Val Loss: 3.1361 | Val Acc: 21.96%\n",
            "Epoch 7/50 | Train Loss: 2.2341 | Train Acc: 42.35% | Val Loss: 2.9927 | Val Acc: 26.27%\n",
            "Epoch 8/50 | Train Loss: 1.8582 | Train Acc: 53.24% | Val Loss: 2.8913 | Val Acc: 28.63%\n",
            "Epoch 9/50 | Train Loss: 1.5323 | Train Acc: 62.16% | Val Loss: 2.8478 | Val Acc: 30.39%\n",
            "Epoch 10/50 | Train Loss: 1.2690 | Train Acc: 68.33% | Val Loss: 2.9729 | Val Acc: 30.39%\n",
            "Epoch 11/50 | Train Loss: 1.0463 | Train Acc: 74.12% | Val Loss: 2.8119 | Val Acc: 32.55%\n",
            "Epoch 12/50 | Train Loss: 0.7622 | Train Acc: 82.55% | Val Loss: 2.7283 | Val Acc: 34.80%\n",
            "Epoch 13/50 | Train Loss: 0.5790 | Train Acc: 87.75% | Val Loss: 2.8664 | Val Acc: 34.12%\n",
            "Epoch 14/50 | Train Loss: 0.4084 | Train Acc: 92.16% | Val Loss: 2.5907 | Val Acc: 38.33%\n",
            "Epoch 15/50 | Train Loss: 0.3034 | Train Acc: 94.71% | Val Loss: 2.7762 | Val Acc: 35.20%\n",
            "Epoch 16/50 | Train Loss: 0.2063 | Train Acc: 97.45% | Val Loss: 2.6695 | Val Acc: 37.65%\n",
            "Epoch 17/50 | Train Loss: 0.1879 | Train Acc: 97.16% | Val Loss: 2.7116 | Val Acc: 40.10%\n",
            "Epoch 18/50 | Train Loss: 0.1612 | Train Acc: 97.35% | Val Loss: 2.6250 | Val Acc: 41.96%\n",
            "Epoch 19/50 | Train Loss: 0.1027 | Train Acc: 98.92% | Val Loss: 2.6713 | Val Acc: 42.35%\n",
            "Epoch 20/50 | Train Loss: 0.0768 | Train Acc: 99.41% | Val Loss: 2.7179 | Val Acc: 40.39%\n",
            "Epoch 21/50 | Train Loss: 0.0514 | Train Acc: 99.71% | Val Loss: 2.6329 | Val Acc: 42.75%\n",
            "Epoch 22/50 | Train Loss: 0.0403 | Train Acc: 99.61% | Val Loss: 2.6334 | Val Acc: 43.63%\n",
            "Epoch 23/50 | Train Loss: 0.0317 | Train Acc: 99.80% | Val Loss: 2.6186 | Val Acc: 43.43%\n",
            "Epoch 24/50 | Train Loss: 0.0218 | Train Acc: 99.80% | Val Loss: 2.6088 | Val Acc: 43.43%\n",
            "Epoch 25/50 | Train Loss: 0.0168 | Train Acc: 100.00% | Val Loss: 2.5321 | Val Acc: 46.18%\n",
            "Epoch 26/50 | Train Loss: 0.0115 | Train Acc: 100.00% | Val Loss: 2.5344 | Val Acc: 46.08%\n",
            "Epoch 27/50 | Train Loss: 0.0097 | Train Acc: 100.00% | Val Loss: 2.5194 | Val Acc: 47.06%\n",
            "Epoch 28/50 | Train Loss: 0.0081 | Train Acc: 100.00% | Val Loss: 2.5116 | Val Acc: 46.47%\n",
            "Epoch 29/50 | Train Loss: 0.0069 | Train Acc: 100.00% | Val Loss: 2.5194 | Val Acc: 46.67%\n",
            "Epoch 30/50 | Train Loss: 0.0062 | Train Acc: 100.00% | Val Loss: 2.5201 | Val Acc: 47.45%\n",
            "Epoch 31/50 | Train Loss: 0.0052 | Train Acc: 100.00% | Val Loss: 2.4964 | Val Acc: 47.45%\n",
            "Epoch 32/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 2.4978 | Val Acc: 48.14%\n",
            "Epoch 33/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 2.5162 | Val Acc: 46.96%\n",
            "Epoch 34/50 | Train Loss: 0.0044 | Train Acc: 100.00% | Val Loss: 2.5120 | Val Acc: 48.04%\n",
            "Epoch 35/50 | Train Loss: 0.0039 | Train Acc: 100.00% | Val Loss: 2.5068 | Val Acc: 47.84%\n",
            "Epoch 36/50 | Train Loss: 0.0040 | Train Acc: 100.00% | Val Loss: 2.5188 | Val Acc: 47.84%\n",
            "Epoch 37/50 | Train Loss: 0.0043 | Train Acc: 100.00% | Val Loss: 2.5459 | Val Acc: 47.35%\n",
            "Early stopping triggered at epoch 37!\n",
            "\n",
            "Experiment 'Alex_wd_0' completed:\n",
            "Best Val Acc: 48.14% | Test Acc: 41.19%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁▂▂▂▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_test_acc</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▄▅▅▆▆▇▇▇███████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▂▂▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▃▃▂▂▃▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>48.13725</td></tr><tr><td>epoch</td><td>36</td></tr><tr><td>final_test_acc</td><td>41.19369</td></tr><tr><td>final_test_loss</td><td>2.90734</td></tr><tr><td>train_acc</td><td>100</td></tr><tr><td>train_loss</td><td>0.00433</td></tr><tr><td>val_acc</td><td>47.35294</td></tr><tr><td>val_loss</td><td>2.54593</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Alex_wd_0</strong> at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/kdr8jvtm' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/kdr8jvtm</a><br> View project at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_151335-kdr8jvtm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 50, 'early_stop_patience': 5, 'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'dropout_p': 0.3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_152215-lefg8xhv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/lefg8xhv' target=\"_blank\">Alex_wd_0.01</a></strong> to <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/lefg8xhv' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/lefg8xhv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Train Loss: 4.6693 | Train Acc: 1.76% | Val Loss: 4.6044 | Val Acc: 4.12%\n",
            "Epoch 2/50 | Train Loss: 4.2725 | Train Acc: 6.18% | Val Loss: 4.3250 | Val Acc: 7.35%\n",
            "Epoch 3/50 | Train Loss: 3.7824 | Train Acc: 12.06% | Val Loss: 3.9334 | Val Acc: 7.55%\n",
            "Epoch 4/50 | Train Loss: 3.3486 | Train Acc: 19.51% | Val Loss: 3.6342 | Val Acc: 12.06%\n",
            "Epoch 5/50 | Train Loss: 2.9909 | Train Acc: 24.61% | Val Loss: 3.3395 | Val Acc: 18.92%\n",
            "Epoch 6/50 | Train Loss: 2.6437 | Train Acc: 32.65% | Val Loss: 3.3255 | Val Acc: 16.67%\n",
            "Epoch 7/50 | Train Loss: 2.3447 | Train Acc: 39.71% | Val Loss: 3.2211 | Val Acc: 20.78%\n",
            "Epoch 8/50 | Train Loss: 2.0393 | Train Acc: 47.25% | Val Loss: 2.9631 | Val Acc: 26.27%\n",
            "Epoch 9/50 | Train Loss: 1.6885 | Train Acc: 58.04% | Val Loss: 2.9920 | Val Acc: 26.18%\n",
            "Epoch 10/50 | Train Loss: 1.4752 | Train Acc: 61.47% | Val Loss: 2.8525 | Val Acc: 28.53%\n",
            "Epoch 11/50 | Train Loss: 1.2381 | Train Acc: 68.82% | Val Loss: 2.8452 | Val Acc: 29.80%\n",
            "Epoch 12/50 | Train Loss: 1.0467 | Train Acc: 74.61% | Val Loss: 2.7981 | Val Acc: 30.39%\n",
            "Epoch 13/50 | Train Loss: 0.8224 | Train Acc: 81.47% | Val Loss: 2.6979 | Val Acc: 34.80%\n",
            "Epoch 14/50 | Train Loss: 0.5902 | Train Acc: 88.92% | Val Loss: 2.6967 | Val Acc: 35.78%\n",
            "Epoch 15/50 | Train Loss: 0.4460 | Train Acc: 93.04% | Val Loss: 2.6786 | Val Acc: 35.59%\n",
            "Epoch 16/50 | Train Loss: 0.3368 | Train Acc: 94.71% | Val Loss: 2.6567 | Val Acc: 37.16%\n",
            "Epoch 17/50 | Train Loss: 0.2570 | Train Acc: 95.98% | Val Loss: 2.6503 | Val Acc: 36.27%\n",
            "Epoch 18/50 | Train Loss: 0.2173 | Train Acc: 97.25% | Val Loss: 2.5133 | Val Acc: 40.59%\n",
            "Epoch 19/50 | Train Loss: 0.1354 | Train Acc: 99.22% | Val Loss: 2.5678 | Val Acc: 41.18%\n",
            "Epoch 20/50 | Train Loss: 0.1111 | Train Acc: 99.51% | Val Loss: 2.4465 | Val Acc: 41.37%\n",
            "Epoch 21/50 | Train Loss: 0.0876 | Train Acc: 99.51% | Val Loss: 2.4856 | Val Acc: 40.88%\n",
            "Epoch 22/50 | Train Loss: 0.0673 | Train Acc: 99.71% | Val Loss: 2.4754 | Val Acc: 42.84%\n",
            "Epoch 23/50 | Train Loss: 0.0519 | Train Acc: 100.00% | Val Loss: 2.4564 | Val Acc: 43.24%\n",
            "Epoch 24/50 | Train Loss: 0.0441 | Train Acc: 99.90% | Val Loss: 2.3506 | Val Acc: 43.92%\n",
            "Epoch 25/50 | Train Loss: 0.0380 | Train Acc: 100.00% | Val Loss: 2.4303 | Val Acc: 42.65%\n",
            "Epoch 26/50 | Train Loss: 0.0369 | Train Acc: 99.90% | Val Loss: 2.2973 | Val Acc: 44.51%\n",
            "Epoch 27/50 | Train Loss: 0.0316 | Train Acc: 100.00% | Val Loss: 2.3027 | Val Acc: 44.61%\n",
            "Epoch 28/50 | Train Loss: 0.0296 | Train Acc: 100.00% | Val Loss: 2.2246 | Val Acc: 45.49%\n",
            "Epoch 29/50 | Train Loss: 0.0298 | Train Acc: 100.00% | Val Loss: 2.2498 | Val Acc: 45.29%\n",
            "Epoch 30/50 | Train Loss: 0.0290 | Train Acc: 100.00% | Val Loss: 2.1891 | Val Acc: 44.61%\n",
            "Epoch 31/50 | Train Loss: 0.0297 | Train Acc: 100.00% | Val Loss: 2.1651 | Val Acc: 46.27%\n",
            "Epoch 32/50 | Train Loss: 0.0306 | Train Acc: 100.00% | Val Loss: 2.1980 | Val Acc: 45.98%\n",
            "Epoch 33/50 | Train Loss: 0.0322 | Train Acc: 100.00% | Val Loss: 2.1970 | Val Acc: 46.76%\n",
            "Epoch 34/50 | Train Loss: 0.0306 | Train Acc: 100.00% | Val Loss: 2.2457 | Val Acc: 46.27%\n",
            "Epoch 35/50 | Train Loss: 0.0351 | Train Acc: 100.00% | Val Loss: 2.1662 | Val Acc: 46.76%\n",
            "Epoch 36/50 | Train Loss: 0.0338 | Train Acc: 100.00% | Val Loss: 2.2123 | Val Acc: 46.18%\n",
            "Epoch 37/50 | Train Loss: 0.0332 | Train Acc: 100.00% | Val Loss: 2.2022 | Val Acc: 45.69%\n",
            "Epoch 38/50 | Train Loss: 0.0351 | Train Acc: 100.00% | Val Loss: 2.1541 | Val Acc: 46.67%\n",
            "Early stopping triggered at epoch 38!\n",
            "\n",
            "Experiment 'Alex_wd_0.01' completed:\n",
            "Best Val Acc: 46.76% | Test Acc: 41.91%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁▂▂▂▃▃▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_test_acc</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▂▂▃▃▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█▇█████████████</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>46.76471</td></tr><tr><td>epoch</td><td>37</td></tr><tr><td>final_test_acc</td><td>41.90925</td></tr><tr><td>final_test_loss</td><td>2.4465</td></tr><tr><td>train_acc</td><td>100</td></tr><tr><td>train_loss</td><td>0.03509</td></tr><tr><td>val_acc</td><td>46.66667</td></tr><tr><td>val_loss</td><td>2.15413</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Alex_wd_0.01</strong> at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/lefg8xhv' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/lefg8xhv</a><br> View project at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_152215-lefg8xhv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 50, 'early_stop_patience': 5, 'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'dropout_p': 0.3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_153113-pit9y3ba</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/pit9y3ba' target=\"_blank\">Alex_wd_0.001</a></strong> to <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/pit9y3ba' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/pit9y3ba</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Train Loss: 4.6612 | Train Acc: 1.67% | Val Loss: 4.6075 | Val Acc: 3.73%\n",
            "Epoch 2/50 | Train Loss: 4.2434 | Train Acc: 6.96% | Val Loss: 4.3279 | Val Acc: 6.08%\n",
            "Epoch 3/50 | Train Loss: 3.7338 | Train Acc: 13.33% | Val Loss: 3.9621 | Val Acc: 11.37%\n",
            "Epoch 4/50 | Train Loss: 3.2359 | Train Acc: 22.16% | Val Loss: 3.6954 | Val Acc: 10.39%\n",
            "Epoch 5/50 | Train Loss: 2.7974 | Train Acc: 29.41% | Val Loss: 3.3366 | Val Acc: 17.16%\n",
            "Epoch 6/50 | Train Loss: 2.4993 | Train Acc: 35.29% | Val Loss: 3.3810 | Val Acc: 18.53%\n",
            "Epoch 7/50 | Train Loss: 2.1222 | Train Acc: 45.39% | Val Loss: 2.8753 | Val Acc: 27.65%\n",
            "Epoch 8/50 | Train Loss: 1.8113 | Train Acc: 54.61% | Val Loss: 2.7512 | Val Acc: 31.86%\n",
            "Epoch 9/50 | Train Loss: 1.5120 | Train Acc: 61.08% | Val Loss: 2.9806 | Val Acc: 26.57%\n",
            "Epoch 10/50 | Train Loss: 1.2454 | Train Acc: 69.80% | Val Loss: 2.6514 | Val Acc: 33.82%\n",
            "Epoch 11/50 | Train Loss: 0.9658 | Train Acc: 76.96% | Val Loss: 3.1059 | Val Acc: 28.33%\n",
            "Epoch 12/50 | Train Loss: 0.7122 | Train Acc: 84.51% | Val Loss: 2.8751 | Val Acc: 32.75%\n",
            "Epoch 13/50 | Train Loss: 0.5217 | Train Acc: 89.71% | Val Loss: 2.6662 | Val Acc: 35.78%\n",
            "Epoch 14/50 | Train Loss: 0.3920 | Train Acc: 92.94% | Val Loss: 2.6637 | Val Acc: 37.16%\n",
            "Epoch 15/50 | Train Loss: 0.2591 | Train Acc: 96.57% | Val Loss: 2.5722 | Val Acc: 38.73%\n",
            "Epoch 16/50 | Train Loss: 0.1572 | Train Acc: 98.73% | Val Loss: 2.5328 | Val Acc: 40.10%\n",
            "Epoch 17/50 | Train Loss: 0.1273 | Train Acc: 98.63% | Val Loss: 2.7252 | Val Acc: 38.14%\n",
            "Epoch 18/50 | Train Loss: 0.1028 | Train Acc: 99.02% | Val Loss: 2.9898 | Val Acc: 36.86%\n",
            "Epoch 19/50 | Train Loss: 0.1041 | Train Acc: 98.63% | Val Loss: 2.8488 | Val Acc: 38.04%\n",
            "Epoch 20/50 | Train Loss: 0.0856 | Train Acc: 99.02% | Val Loss: 2.6594 | Val Acc: 39.02%\n",
            "Epoch 21/50 | Train Loss: 0.0549 | Train Acc: 99.80% | Val Loss: 2.4836 | Val Acc: 44.02%\n",
            "Epoch 22/50 | Train Loss: 0.0391 | Train Acc: 99.90% | Val Loss: 2.7403 | Val Acc: 41.37%\n",
            "Epoch 23/50 | Train Loss: 0.0305 | Train Acc: 100.00% | Val Loss: 2.4971 | Val Acc: 42.25%\n",
            "Epoch 24/50 | Train Loss: 0.0197 | Train Acc: 100.00% | Val Loss: 2.5745 | Val Acc: 44.12%\n",
            "Epoch 25/50 | Train Loss: 0.0166 | Train Acc: 100.00% | Val Loss: 2.5145 | Val Acc: 45.20%\n",
            "Epoch 26/50 | Train Loss: 0.0127 | Train Acc: 100.00% | Val Loss: 2.4805 | Val Acc: 44.02%\n",
            "Epoch 27/50 | Train Loss: 0.0118 | Train Acc: 100.00% | Val Loss: 2.4287 | Val Acc: 45.88%\n",
            "Epoch 28/50 | Train Loss: 0.0090 | Train Acc: 100.00% | Val Loss: 2.4617 | Val Acc: 46.08%\n",
            "Epoch 29/50 | Train Loss: 0.0078 | Train Acc: 100.00% | Val Loss: 2.4259 | Val Acc: 45.98%\n",
            "Epoch 30/50 | Train Loss: 0.0074 | Train Acc: 100.00% | Val Loss: 2.4190 | Val Acc: 46.86%\n",
            "Epoch 31/50 | Train Loss: 0.0063 | Train Acc: 100.00% | Val Loss: 2.4094 | Val Acc: 46.67%\n",
            "Epoch 32/50 | Train Loss: 0.0072 | Train Acc: 100.00% | Val Loss: 2.4711 | Val Acc: 45.49%\n",
            "Epoch 33/50 | Train Loss: 0.0081 | Train Acc: 99.90% | Val Loss: 2.4745 | Val Acc: 45.29%\n",
            "Epoch 34/50 | Train Loss: 0.0114 | Train Acc: 99.90% | Val Loss: 2.5168 | Val Acc: 44.80%\n",
            "Epoch 35/50 | Train Loss: 0.0127 | Train Acc: 99.90% | Val Loss: 2.6371 | Val Acc: 42.84%\n",
            "Early stopping triggered at epoch 35!\n",
            "\n",
            "Experiment 'Alex_wd_0.001' completed:\n",
            "Best Val Acc: 46.86% | Test Acc: 38.92%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁▁▂▂▃▃▅▆▆▆▆▆▆▆▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_test_acc</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▄▅▅▆▆▇▇▇█████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▂▃▃▅▆▅▆▅▆▆▆▇▇▇▆▇▇█▇▇███████████▇</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▂▂▃▂▃▂▂▂▂▁▂▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>46.86275</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>final_test_acc</td><td>38.9169</td></tr><tr><td>final_test_loss</td><td>2.89723</td></tr><tr><td>train_acc</td><td>99.90196</td></tr><tr><td>train_loss</td><td>0.01266</td></tr><tr><td>val_acc</td><td>42.84314</td></tr><tr><td>val_loss</td><td>2.63708</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Alex_wd_0.001</strong> at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/pit9y3ba' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/pit9y3ba</a><br> View project at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_153113-pit9y3ba/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 50, 'early_stop_patience': 5, 'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'dropout_p': 0.3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_153933-zdlkbt10</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/zdlkbt10' target=\"_blank\">Alex_wd_0.0001</a></strong> to <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/zdlkbt10' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/zdlkbt10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Train Loss: 4.6649 | Train Acc: 1.18% | Val Loss: 4.5993 | Val Acc: 6.18%\n",
            "Epoch 2/50 | Train Loss: 4.2377 | Train Acc: 6.18% | Val Loss: 4.2694 | Val Acc: 7.75%\n",
            "Epoch 3/50 | Train Loss: 3.7378 | Train Acc: 14.51% | Val Loss: 3.8361 | Val Acc: 12.45%\n",
            "Epoch 4/50 | Train Loss: 3.3052 | Train Acc: 19.61% | Val Loss: 3.6070 | Val Acc: 13.53%\n",
            "Epoch 5/50 | Train Loss: 2.9373 | Train Acc: 28.92% | Val Loss: 3.4276 | Val Acc: 17.55%\n",
            "Epoch 6/50 | Train Loss: 2.6174 | Train Acc: 34.22% | Val Loss: 3.3867 | Val Acc: 19.71%\n",
            "Epoch 7/50 | Train Loss: 2.2551 | Train Acc: 42.16% | Val Loss: 3.0885 | Val Acc: 22.65%\n",
            "Epoch 8/50 | Train Loss: 1.9831 | Train Acc: 47.94% | Val Loss: 2.8687 | Val Acc: 29.22%\n",
            "Epoch 9/50 | Train Loss: 1.6602 | Train Acc: 56.96% | Val Loss: 2.9307 | Val Acc: 28.63%\n",
            "Epoch 10/50 | Train Loss: 1.3322 | Train Acc: 65.59% | Val Loss: 3.0849 | Val Acc: 29.31%\n",
            "Epoch 11/50 | Train Loss: 1.0995 | Train Acc: 73.73% | Val Loss: 2.8934 | Val Acc: 30.00%\n",
            "Epoch 12/50 | Train Loss: 0.9123 | Train Acc: 77.65% | Val Loss: 2.9175 | Val Acc: 33.04%\n",
            "Epoch 13/50 | Train Loss: 0.7159 | Train Acc: 82.45% | Val Loss: 2.8265 | Val Acc: 34.80%\n",
            "Epoch 14/50 | Train Loss: 0.5251 | Train Acc: 87.06% | Val Loss: 2.8538 | Val Acc: 35.69%\n",
            "Epoch 15/50 | Train Loss: 0.3622 | Train Acc: 93.82% | Val Loss: 2.6832 | Val Acc: 37.35%\n",
            "Epoch 16/50 | Train Loss: 0.2670 | Train Acc: 95.00% | Val Loss: 2.8659 | Val Acc: 35.98%\n",
            "Epoch 17/50 | Train Loss: 0.1995 | Train Acc: 97.75% | Val Loss: 2.6790 | Val Acc: 38.63%\n",
            "Epoch 18/50 | Train Loss: 0.1364 | Train Acc: 98.24% | Val Loss: 2.7862 | Val Acc: 36.08%\n",
            "Epoch 19/50 | Train Loss: 0.0914 | Train Acc: 99.02% | Val Loss: 2.8593 | Val Acc: 38.24%\n",
            "Epoch 20/50 | Train Loss: 0.0673 | Train Acc: 99.80% | Val Loss: 2.5165 | Val Acc: 42.94%\n",
            "Epoch 21/50 | Train Loss: 0.0467 | Train Acc: 99.90% | Val Loss: 2.6840 | Val Acc: 39.51%\n",
            "Epoch 22/50 | Train Loss: 0.0337 | Train Acc: 100.00% | Val Loss: 2.5813 | Val Acc: 43.92%\n",
            "Epoch 23/50 | Train Loss: 0.0241 | Train Acc: 99.90% | Val Loss: 2.5290 | Val Acc: 44.31%\n",
            "Epoch 24/50 | Train Loss: 0.0183 | Train Acc: 100.00% | Val Loss: 2.5694 | Val Acc: 43.43%\n",
            "Epoch 25/50 | Train Loss: 0.0143 | Train Acc: 100.00% | Val Loss: 2.4796 | Val Acc: 44.31%\n",
            "Epoch 26/50 | Train Loss: 0.0111 | Train Acc: 100.00% | Val Loss: 2.4693 | Val Acc: 43.53%\n",
            "Epoch 27/50 | Train Loss: 0.0098 | Train Acc: 100.00% | Val Loss: 2.4695 | Val Acc: 44.41%\n",
            "Epoch 28/50 | Train Loss: 0.0075 | Train Acc: 100.00% | Val Loss: 2.4860 | Val Acc: 45.39%\n",
            "Epoch 29/50 | Train Loss: 0.0077 | Train Acc: 100.00% | Val Loss: 2.4560 | Val Acc: 44.61%\n",
            "Epoch 30/50 | Train Loss: 0.0066 | Train Acc: 100.00% | Val Loss: 2.4538 | Val Acc: 45.10%\n",
            "Epoch 31/50 | Train Loss: 0.0063 | Train Acc: 100.00% | Val Loss: 2.4788 | Val Acc: 45.39%\n",
            "Epoch 32/50 | Train Loss: 0.0071 | Train Acc: 100.00% | Val Loss: 2.4936 | Val Acc: 46.18%\n",
            "Epoch 33/50 | Train Loss: 0.0058 | Train Acc: 100.00% | Val Loss: 2.4817 | Val Acc: 45.78%\n",
            "Epoch 34/50 | Train Loss: 0.0053 | Train Acc: 100.00% | Val Loss: 2.4976 | Val Acc: 45.69%\n",
            "Epoch 35/50 | Train Loss: 0.0051 | Train Acc: 100.00% | Val Loss: 2.5009 | Val Acc: 44.71%\n",
            "Epoch 36/50 | Train Loss: 0.0042 | Train Acc: 100.00% | Val Loss: 2.4893 | Val Acc: 45.88%\n",
            "Epoch 37/50 | Train Loss: 0.0043 | Train Acc: 100.00% | Val Loss: 2.4650 | Val Acc: 46.08%\n",
            "Early stopping triggered at epoch 37!\n",
            "\n",
            "Experiment 'Alex_wd_0.0001' completed:\n",
            "Best Val Acc: 46.18% | Test Acc: 41.81%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁▁▂▂▃▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_test_acc</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▄▄▅▆▆▆▇▇███████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▂▃▃▄▅▅▅▅▆▆▆▆▆▇▆▇▇▇████████████████</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▃▂▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>46.17647</td></tr><tr><td>epoch</td><td>36</td></tr><tr><td>final_test_acc</td><td>41.81168</td></tr><tr><td>final_test_loss</td><td>2.75808</td></tr><tr><td>train_acc</td><td>100</td></tr><tr><td>train_loss</td><td>0.00432</td></tr><tr><td>val_acc</td><td>46.07843</td></tr><tr><td>val_loss</td><td>2.46497</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Alex_wd_0.0001</strong> at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/zdlkbt10' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/zdlkbt10</a><br> View project at: <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_153933-zdlkbt10/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 50, 'early_stop_patience': 5, 'batch_size': 128, 'learning_rate': 0.0001, 'weight_decay': 1e-05, 'dropout_p': 0.3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_154816-gci1zia8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/gci1zia8' target=\"_blank\">Alex_wd_1e-05</a></strong> to <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/gci1zia8' target=\"_blank\">https://wandb.ai/andrzej-pijanowski-uniwroc/flowers102-classification/runs/gci1zia8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Train Loss: 4.6769 | Train Acc: 1.47% | Val Loss: 4.6026 | Val Acc: 5.20%\n",
            "Epoch 2/50 | Train Loss: 4.2609 | Train Acc: 6.47% | Val Loss: 4.2842 | Val Acc: 7.16%\n",
            "Epoch 3/50 | Train Loss: 3.7128 | Train Acc: 13.92% | Val Loss: 3.8601 | Val Acc: 15.20%\n",
            "Epoch 4/50 | Train Loss: 3.2247 | Train Acc: 20.88% | Val Loss: 3.6798 | Val Acc: 11.37%\n",
            "Epoch 5/50 | Train Loss: 2.8882 | Train Acc: 27.35% | Val Loss: 3.3461 | Val Acc: 16.86%\n",
            "Epoch 6/50 | Train Loss: 2.5560 | Train Acc: 33.53% | Val Loss: 3.2687 | Val Acc: 19.02%\n",
            "Epoch 7/50 | Train Loss: 2.2489 | Train Acc: 38.43% | Val Loss: 3.3235 | Val Acc: 19.41%\n",
            "Epoch 8/50 | Train Loss: 1.9209 | Train Acc: 50.49% | Val Loss: 3.0810 | Val Acc: 23.14%\n",
            "Epoch 9/50 | Train Loss: 1.5840 | Train Acc: 57.94% | Val Loss: 2.9076 | Val Acc: 31.57%\n",
            "Epoch 10/50 | Train Loss: 1.2565 | Train Acc: 67.35% | Val Loss: 2.6189 | Val Acc: 33.14%\n",
            "Epoch 11/50 | Train Loss: 1.0311 | Train Acc: 74.31% | Val Loss: 3.0633 | Val Acc: 29.80%\n",
            "Epoch 12/50 | Train Loss: 0.8316 | Train Acc: 80.10% | Val Loss: 2.7195 | Val Acc: 34.61%\n",
            "Epoch 13/50 | Train Loss: 0.6362 | Train Acc: 85.88% | Val Loss: 2.6735 | Val Acc: 35.88%\n",
            "Epoch 14/50 | Train Loss: 0.4865 | Train Acc: 89.22% | Val Loss: 2.6610 | Val Acc: 36.37%\n",
            "Epoch 15/50 | Train Loss: 0.3473 | Train Acc: 94.22% | Val Loss: 2.7323 | Val Acc: 38.43%\n",
            "Epoch 16/50 | Train Loss: 0.2581 | Train Acc: 96.27% | Val Loss: 2.6690 | Val Acc: 35.88%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d9b117b94945>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m                                \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                weight_decay=exp['wd_config']['weight_decay'])\n\u001b[0;32m--> 108\u001b[0;31m         train_model(model,\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mbase_train_loader_alex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-75b7d06fbabc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, val_loader, test_loader, experiment_name, group, config, criterion, additional_tags)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-47de130667a4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "experiments_config = {\n",
        "    'lr_test': False,\n",
        "    'lr_adam_test': False,\n",
        "    'lr_more_epochs_test': False,\n",
        "    'lr_adam_more_epochs_test': False,\n",
        "    'dropout_test': False,\n",
        "    'weight_decay_test': False,\n",
        "    'data_augmentation': True\n",
        "}\n",
        "\n",
        "if experiments_config['lr_test']:\n",
        "    for exp in lr_experiments:\n",
        "        model = AlexNet().to(device)\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=exp['lr_config']['learning_rate'],\n",
        "                              weight_decay=exp['lr_config']['weight_decay'],\n",
        "                              momentum=exp['lr_config']['momentum'])\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['lr_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['lr_adam_test']:\n",
        "    for exp in lr_adam_experiments:\n",
        "        model = AlexNet().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=exp['lr_config']['learning_rate'],\n",
        "                               weight_decay=exp['lr_config']['weight_decay'])\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['lr_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['lr_more_epochs_test']:\n",
        "    for exp in lr_more_epochs_experiments:\n",
        "        model = AlexNet().to(device)\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=exp['lr_config']['learning_rate'],\n",
        "                              weight_decay=exp['lr_config']['weight_decay'],\n",
        "                              momentum=exp['lr_config'].get('momentum', 0.9))\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['lr_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['lr_adam_more_epochs_test']:\n",
        "    for exp in lr_adam_more_epochs_experiments:\n",
        "        model = AlexNet(dropout=exp['lr_config']['dropout_p']).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=exp['lr_config']['learning_rate'],\n",
        "                               weight_decay=exp['lr_config']['weight_decay'])\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['lr_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['dropout_test']:\n",
        "    for exp in dr_experiments:\n",
        "        model = AlexNet(dropout=exp['dr_config']['dropout_p']).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=exp['dr_config']['learning_rate'],\n",
        "                               weight_decay=exp['dr_config']['weight_decay'])\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['dr_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['weight_decay_test']:\n",
        "    for exp in wd_experiments:\n",
        "        model = AlexNet(dropout=exp['wd_config']['dropout_p']).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=exp['wd_config']['learning_rate'],\n",
        "                               weight_decay=exp['wd_config']['weight_decay'])\n",
        "        train_model(model,\n",
        "                    optimizer,\n",
        "                    base_train_loader_alex,\n",
        "                    base_val_loader_alex,\n",
        "                    base_test_loader_alex,\n",
        "                    exp['experiment_name'],\n",
        "                    exp['group'],\n",
        "                    exp['wd_config'],\n",
        "                    additional_tags=exp['additional_tags'])\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if experiments_config['data_augmentation']:\n",
        "    model = AlexNet(dropout=base_config_adam['dropout_p']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                            lr=base_config_adam['learning_rate'],\n",
        "                            weight_decay=base_config_adam['weight_decay'])\n",
        "    train_model(model,\n",
        "                optimizer,\n",
        "                augmented_train_loader_alex,\n",
        "                base_val_loader_alex,\n",
        "                base_test_loader_alex,\n",
        "                'Alex_data_augmentation',\n",
        "                'data_augmentation',\n",
        "                base_config_adam,\n",
        "                additional_tags=['data_augmentation'])\n",
        "    del model, optimizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model = AlexNet(dropout=base_config_adam['dropout_p']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                            lr=base_config_adam['learning_rate'],\n",
        "                            weight_decay=base_config_adam['weight_decay'])\n",
        "    train_model(model,\n",
        "                optimizer,\n",
        "                base_train_loader_alex,\n",
        "                base_val_loader_alex,\n",
        "                base_test_loader_alex,\n",
        "                'Alex_data_augmentation_no_aug',\n",
        "                'data_augmentation',\n",
        "                base_config_adam,\n",
        "                additional_tags=['data_augmentation', 'no_aug'])\n",
        "    del model, optimizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}